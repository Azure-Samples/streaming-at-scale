# Streaming at Scale integration tests

## Integration test suite

The suite is designed to run in a fully automated manner in Azure DevOps. It
deploys each scenario from the streaming-at-scale project in a new resource
group, then runs a verification job in Azure Databricks that reads the entire
set of events generated by the scenario, and tests assertions on the throughput
and latency. For instance, after running the eventhubs-functions-cosmosdb
scenario at 5000 events/second, the verification job downloads the output data
stored in CosmosDB and computes the end-to-end latency and throughput
aggregated at one-minute interval. To account for some runtime variability, the
test asserts that a throughput of at least 90% of the expected throughput (in
that case, 270,000 events in a one-minute interval) was reached at least once.

Since the provisioning of an Azure Databricks workspace cannot be fully
automated at present, you must generate a PAT token of a preexisting workspace
and supply it to the pipeline.

## Creating the integration test pipeline in Azure DevOps

* Create a Databricks workspace in the Azure region of your choice.
* Create a project in Azure Pipelines.
* In your Azure Pipelines project settings, navigate to service connection and
  create an ARM service connection to your Azure subscription named
  'ARMConnection'. Do not restrict the connection to a particular resource
  group.
* Create a build pipeline:
  * As pipeline source, use https://github.com/Azure-Samples/streaming-at-scale.git.
  * As pipeline content, reference integration-tests/azure-pipelines.yaml.
  * Define the following  pipeline variables:

| Variable name          | Description                                    | Required? | Example    |
| --------------------   | ---------------------------------------------- | --------- | ---------- |
| LOCATION               | Azure region in which to deploy infrastructure | required  | eastus     |
| DATABRICKS_PAT_TOKEN   | (secret variable) Databricks PAT token for a Databricks workspace deployed in $LOCATION | required | dapi01234567890123456789012345678901 |
| RESOURCE_GROUP_PREFIX  | Prefix used to name deployed resources. Must be globally unique, use a sufficiently unique string  | required | xyzzy0x4 |


## Running the integration tests

Running the pipeline will generate multiple runner jobs, and execute them in
sequence. Each scenario is deployed in a newly created resource group, that is
immediately deleted after the test is run.

At the end of the job output of the 'Run test' task is a link to the Databricks
job run URL for the verification job. You can open this URL to view the run
notebook output. At the end of the main notebook is a call to the
'verify-common' notebook job, open that notebook run to view detailed
statistics collected by the verification job.

