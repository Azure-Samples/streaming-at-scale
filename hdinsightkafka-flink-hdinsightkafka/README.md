---
topic: sample
languages:
  - azurecli
  - json
  - sql
products:
  - azure
  - azure-container-instances
  - azure-container-service
  - azure-hdinsight
statusNotificationTargets:
  - algattik@microsoft.com
---

# Streaming at Scale with Azure HDInsight Kafka and Apache Flink

This sample uses Apache Flink to process streaming data from HDInsight Kafka and uses another topic in HDInsight Kafka as a sink to store JSON data. This is done to analyze pure streaming performance of Flink; no aggregation is done and data is passed as fast as possible from the input to the output. Data is augmented by adding additional fields.

The sample provides a choice among options for hosting Flink: Azure Kubernetes Service, or Azure HDInsight Hadoop (using YARN).

The provided scripts will create an end-to-end solution complete with load test client.

## Running the Scripts

Please note that the scripts have been tested on [Ubuntu 18 LTS](http://releases.ubuntu.com/18.04/), so make sure to use that environment to run the scripts. You can run it using Docker, WSL or a VM:

- [Ubuntu Docker Image](https://hub.docker.com/_/ubuntu/)
- [WSL Ubuntu 18.04 LTS](https://www.microsoft.com/en-us/p/ubuntu-1804-lts/9n9tngvndl3q?activetab=pivot:overviewtab)
- [Ubuntu 18.04 LTS Azure VM](https://azuremarketplace.microsoft.com/en-us/marketplace/apps/Canonical.UbuntuServer1804LTS)

The following tools/languages are also needed:

- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt?view=azure-cli-latest)
  - Install: `sudo apt install azure-cli`
- [jq](https://stedolan.github.io/jq/)
  - Install: `sudo apt install jq`
- [Maven](https://maven.apache.org/install.html)
  - Install: `sudo apt install maven`
- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)
- [helm](https://helm.sh/docs/using_helm/#installing-helm)

## Setup Solution

Make sure you are logged into your Azure account:

    az login

and also make sure you have the subscription you want to use selected

    az account list

if you want to select a specific subscription use the following command

    az account set --subscription <subscription_name>

once you have selected the subscription you want to use just execute the following command

    ./create-solution.sh -d <solution_name>

then `solution_name` value will be used to create a resource group that will contain all resources created by the script. It will also be used as a prefix for all resource create so, in order to help to avoid name duplicates that will break the script, you may want to generated a name using a unique prefix. **Please also use only lowercase letters and numbers only**, since the `solution_name` is also used to create a storage account, which has several constraints on characters usage:

[Storage Naming Conventions and Limits](https://docs.microsoft.com/en-us/azure/architecture/best-practices/naming-conventions#storage)

to have an overview of all the supported arguments just run

    ./create-solution.sh

**Note**
To make sure that name collisions will be unlikely, you should use a random string to give name to your solution. The following script will generated a 7 random lowercase letter name for you:

    ./generate-solution-name.sh

## Created resources

The script will create the following resources:

- **Azure Container Instances** to host Spark Load Test Clients: by default one client will be created, generating a load of 1000 events/second
- **HDInsight Kafka** cluster: to ingest data incoming from test clients and to store data generated by Apache Flink
- **HDInsight** or **Azure Kubernetes Service**: to host the Apache Flink job that processes event data
- **Azure Monitor**: to monitor HDInsight, Azure Kubernetes Service and Flink

## Streamed Data

Streamed data simulates an IoT device sending the following JSON data:

```json
{
    "eventId": "b81d241f-5187-40b0-ab2a-940faf9757c0",
    "complexData": {
        "moreData0": 57.739726013343247,
        "moreData1": 52.230732688620829,
        "moreData2": 57.497518587807189,
        "moreData3": 81.32211656749469,
        "moreData4": 54.412361539409427,
        "moreData5": 75.36416309399911,
        "moreData6": 71.53407865773488,
        "moreData7": 45.34076957651598,
        "moreData8": 51.3068118685458,
        "moreData9": 44.44672606436184,
        [...]
    },
    "value": 49.02278128887753,
    "deviceId": "contoso://device-id-154",
    "deviceSequenceNumber": 0,
    "type": "CO2",
    "createdAt": "2019-05-16T17:16:40.000003Z"
}
```

## Duplicate event handling

The solution does not perform event deduplication. In order to illustrate the effect of this, the event simulator is configured to randomly duplicate a small fraction of the messages (0.1% on average). Those duplicate events will be present in the destination Kafka topic.

## Solution customization

If you want to change some setting of the solution, like number of load test clients, event hubs TU and so on, you can do it right in the `create-solution.sh` script, by changing any of these values:

    export HDINSIGHT_KAFKA_WORKERS=4
    export HDINSIGHT_KAFKA_WORKER_SIZE=Standard_D3_V2
    export KAFKA_PARTITIONS=1
    export FLINK_PARALLELISM=1
    export SIMULATOR_INSTANCES=1
    # settings for AKS (-p aks)
    export AKS_NODES=3
    export AKS_VM_SIZE=Standard_D2s_v3
    # settings for HDInsight YARN (-p hdinsight)
    export HDINSIGHT_HADOOP_WORKERS=3
    export HDINSIGHT_HADOOP_WORKER_SIZE=Standard_D3_V2

The above settings have been chosen to sustain a 1,000 msg/s stream. The script also contains settings for 5,000 msg/s and 10,000 msg/s.

## Monitor performance

The deployment script will report performance, by default every minute for 30 minutes:

```
***** [M] Starting METRICS reporting
getting HDInsight Ambari endpoint...
reading Kafka Broker IPs from HDInsight Ambari...
Reporting aggregate metrics per minute, for 30 minutes.
                                IncomingMessages       IncomingBytes       OutgoingBytes
                                ----------------       -------------       -------------
    2019-11-17T18:41:34+0100                   0                   0                   0
    2019-11-17T18:42:01+0100                   0                   0                   0
    2019-11-17T18:43:01+0100                   0                   0                   0
    2019-11-17T18:44:01+0100                   0                   0                   0
    2019-11-17T18:45:01+0100               37092            34457235                   0
    2019-11-17T18:46:01+0100               37092            34457235                   0
    2019-11-17T18:47:01+0100               56950            53014251                   0
    2019-11-17T18:48:00+0100               56950            53014251                   0
    2019-11-17T18:49:02+0100               95568            91987286            33357210
    2019-11-17T18:50:00+0100               95568            91987286            33357210
    2019-11-17T18:51:01+0100              116900           113538933            52875620
    2019-11-17T18:52:01+0100              116900           113538933            52875620
    2019-11-17T18:53:01+0100              119814           116464381            55520578
    2019-11-17T18:54:00+0100              119814           116464381            55520578
    2019-11-17T18:55:01+0100              120196           116870496            55887922
```

Allowing for a few minutes of ramp-up, incoming rate will stabilize at twice the event generation rate, since every event is ingested once and then again after Flink processing.

## Apache Flink

The deployed Apache Flink solution doesn't do any analytics or projection, but only populates two fields in the JSON message: the time at which the event was received in HDInsight Kafka, and the current timestamp.

The solution includes a custom monitoring library to log Flink events and metrics to Azure Monitor. The custom monitoring library is currently only included when the Flink job is deployed in AKS. To view the monitoring data, navigate to the Log Analytics resource in the Azure Portal.

The Flink Job Manager UI shows information about the current running job. The IP address of the Job Manager UI is reported by the deployment script. Note that the solution deploys the Job Manager on a public IP address without any security. In a production deployment, you should disable public IP endpoints.

![Flink Job Manager Web UI](../_doc/_images/flink-job-manager.png)

### Flink deployment on AKS

Deployment on Azure Kubernetes Service is done in single-job, highly available mode. The deployment includes:
* A Zookeeper cluster for maintaining quorum
* A pod for the (per-job) Flink Job Manager and 
* A pod for each Flink Task Manager deployed as part of the job

In HA mode, the Flink the JobManager exposes a dynamically allocated port. Together with the JobManager, we run a custom sidecar container containing a small shell script. The script calls the JobManager REST API (running on fixed port 8081) to discover the JobManager RPC port, then calls the Kubernetes API to update the port exposed in the Kubernetes Service. RBAC is used to grant the sidecar container permissions to only this specific operation in the API.

### Flink deployment on HDInsight

Deployment on HDInsight is done in job server, highly available mode. The deployment runs a YARN job for the Flink Job Manager, then submits a JAR job to the Job Manager. The Job Manager creates a YARN application per job.

Note that deployed jobs do not survive an HDInsight cluster reboot.

## Clean up

To remove all the created resource, you can just delete the related resource group:

```bash
az group delete -n <resource-group-name>
```
